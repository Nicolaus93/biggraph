{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tPolitical ideologies\tSocial theories\tPolitical culture\tAnti-capitalism\tFar-left politics\tAnarchism\n",
      "['anarch', 'polit', 'ideolog', 'social', 'theori', 'polit', 'cultur', 'anti-capit', 'far-left', 'polit', 'anarch']\n",
      "Değnek\tVillages by country\tRegions of Turkey\tPopulated places in Turkey by province\n",
      "['değnek', 'villag', 'countri', 'region', 'turkey', 'popul', 'place', 'turkey', 'provinc']\n",
      "Queensland Conservatorium Griffith University\tUniversities by country\tEntertainment in Australia\tAustralian capital cities\n",
      "['queensland', 'conservatorium', 'griffith', 'univers', 'univers', 'countri', 'entertain', 'australia', 'australian', 'capit', 'citi']\n",
      "Octagon Chapel, Liverpool\tChurches\tBuildings and structures in England by city\n",
      "['octagon', 'chapel', ',', 'liverpool', 'church', 'build', 'structur', 'england', 'citi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "basename = \"enwiki-2016\"\n",
    "cat_file = Path(\"/data/graphs\") / basename / \"page2cat.tsv\"\n",
    "cat_list = []\n",
    "ps = PorterStemmer()\n",
    "for pos, line in enumerate(cat_file.open(encoding=\"utf-8\")):\n",
    "    temp = line.rstrip()\n",
    "    words = word_tokenize(temp)\n",
    "    final = [ps.stem(i) for i in words if i not in stopwords.words('english')]\n",
    "    print(temp)\n",
    "    print(final)\n",
    "    if pos > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_cat_dict(cat_file):\n",
    "    categories = defaultdict(list)\n",
    "    for pos, line in enumerate(cat_file.open(encoding=\"utf-8\")):\n",
    "        temp = line.rstrip().split(\"\\t\", 1)\n",
    "        try:\n",
    "            key, raw_cat = temp[0], temp[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        categories[key] = preprocess_categs(raw_cat)\n",
    "        if pos > 100:\n",
    "            break\n",
    "    return categories\n",
    "\n",
    "\n",
    "def preprocess_categs(raw_categs):\n",
    "    words = word_tokenize(raw_categs)\n",
    "    doc = []\n",
    "    for i in words:\n",
    "        if i not in stopwords.words('english'):\n",
    "            doc.append(ps.stem(i))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f build_cat_dict build_cat_dict(cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f preprocess_categs build_cat_dict(cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categs(raw_categs):\n",
    "    words = word_tokenize(raw_categs)\n",
    "    return [ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f preprocess_categs build_cat_dict(cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cat_dict(cat_file):\n",
    "    categories = defaultdict(list)\n",
    "    stopw = set(stopwords.words('english'))\n",
    "    for pos, line in enumerate(cat_file.open(encoding=\"utf-8\")):\n",
    "        temp = line.rstrip().split(\"\\t\", 1)\n",
    "        try:\n",
    "            key, raw_cat = temp[0], temp[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        categories[key] = preprocess_categs(raw_cat, stopw)\n",
    "        if pos > 100:\n",
    "            break\n",
    "    return categories\n",
    "\n",
    "\n",
    "def preprocess_categs(raw_categs, stopwords):\n",
    "    words = word_tokenize(raw_categs)\n",
    "    return [ps.stem(w) for w in words if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f preprocess_categs build_cat_dict(cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_cat_dict(cat_file):\n",
    "    categories = defaultdict(list)\n",
    "    stopw = set(stopwords.words('english'))\n",
    "    num_lines = sum(1 for _ in cat_file.open(encoding=\"utf-8\"))\n",
    "    for pos, line in enumerate(tqdm(cat_file.open(encoding=\"utf-8\"), total=num_lines)):\n",
    "        temp = line.rstrip().split(\"\\t\", 1)\n",
    "        try:\n",
    "            key, raw_cat = temp[0], temp[1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        categories[key] = preprocess_categs(raw_cat, stopw)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 792668/4289768 [06:31<28:31, 2042.79it/s]"
     ]
    }
   ],
   "source": [
    "categs = build_cat_dict(cat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3192605"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['polit',\n",
       " 'ideolog',\n",
       " 'social',\n",
       " 'theori',\n",
       " 'polit',\n",
       " 'cultur',\n",
       " 'anti-capit',\n",
       " 'far-left',\n",
       " 'polit',\n",
       " 'anarch']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categs['Anarchism']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3192605"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [' '.join(i) for i in categs.values()]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polit ideolog social theori polit cultur anti-capit far-left polit anarch\n",
      "villag countri region turkey popul place turkey provinc\n",
      "univers countri entertain australia australian capit citi\n",
      "church build structur england citi\n",
      "peopl statu peopl ethnic peopl ethnic occup indian peopl film director\n",
      "scottish societi alumni univers colleg europ poetri nation languag scottish peopl occup\n",
      "plant\n"
     ]
    }
   ],
   "source": [
    "for pos, value in enumerate(docs):\n",
    "    print(' '.join(value))\n",
    "    if pos > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(docs)\n",
    "type(sklearn_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3192605, 2461)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "x1 = sklearn_representation[0]\n",
    "x2 = sklearn_representation[1]\n",
    "cosine_similarity(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning it right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "docs_list = []\n",
    "for key, value in categs.items():\n",
    "    key_list.append(key)\n",
    "    docs_list.append(' '.join(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_repr = sklearn_tfidf.fit_transform(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "p = Path('.').resolve()\n",
    "sys.path.append(str(p.parent))\n",
    "from utils.data_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n",
      "Labels not defined\n"
     ]
    }
   ],
   "source": [
    "x, y = load_data(Path(\"/data/models/enwiki-2016\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "def get_id(basename, idx, ent_list):\n",
    "    \"\"\"\n",
    "    Idx is currently a list.\n",
    "    \"\"\"\n",
    "    ids_file = Path('/data/graphs/') / basename / (basename + '.urls')\n",
    "    if not ids_file.exists():\n",
    "        ids_file = Path('/data/graphs/') / basename / (basename + '.ids')\n",
    "    assert ids_file.exists(), \"File not found!\"\n",
    "    f = ids_file.as_posix()\n",
    "    for node in idx:\n",
    "        line = ent_list[node]\n",
    "        yield linecache.getline(f, line + 1).rstrip()\n",
    "\n",
    "\n",
    "\n",
    "def cosineSim_vs_distance(x, categories, entities, key_list, doc_list, n=100):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    subset = np.random.choice(len(x), n, replace=False)\n",
    "    ids = [i for i in get_id(\"enwiki-2016\", subset, entities)]\n",
    "    # polishing\n",
    "    print(\"Removing nodes not contained in the categories dictionary..\")\n",
    "    indices = []\n",
    "    for pos, value in enumerate(ids):\n",
    "        if value in categories:\n",
    "            if len(categories[value]) > 0: # this should always be true\n",
    "                indices.append(pos)\n",
    "    # keep only relevant ids and associated embeddings\n",
    "    ids = [ids[i] for i in indices]\n",
    "    subset = [subset[i] for i in indices]\n",
    "    print(\"Effective number of nodes: {}\".format(len(ids)))\n",
    "    # compute\n",
    "    mat = []\n",
    "    for i in ids:\n",
    "        idx = key_list.index(i)\n",
    "        mat.append(doc_list[idx])\n",
    "    print(len(mat))\n",
    "    mat = scipy.sparse.vstack(mat)\n",
    "    similarities_sparse = cosine_similarity(mat, dense_output=False)\n",
    "    dist = pdist(x[subset], 'euclidean')\n",
    "    return similarities_sparse, dist, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing nodes not contained in the categories dictionary..\n",
      "Effective number of nodes: 7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "temp1, temp2, temp3 = cosineSim_vs_distance(x, categs, entities, key_list, doc_repr, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.1757733968820497\n",
      "  (2, 5)\t0.13781724510952068\n"
     ]
    }
   ],
   "source": [
    "print(scipy.sparse.triu(temp1, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_x = scipy.sparse.triu(temp1, k=1)\n",
    "plt.scatter(, js, s=1)\n",
    "plt.xlabel(\"L2 distance\")\n",
    "plt.ylabel(\"Jaccard sym\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
